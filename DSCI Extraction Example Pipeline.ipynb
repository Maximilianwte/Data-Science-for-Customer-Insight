{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91023762-c017-4afd-9ef5-c0865bff05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f0158-e00c-499f-a8ff-6990b74b0400",
   "metadata": {},
   "source": [
    "## 1. Getting an article with the SQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153cd1c-5a96-4cd1-8790-593165cfb1a9",
   "metadata": {},
   "source": [
    "To easily access all of our data, we have combined all the articles as text into a SQL database. First connect to the database by putting the file into your google drive or google colab and then access any of the articles in it by querying the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f98ec9-c55e-4c0b-82af-d665fbafc089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('/content/db_bwl.db')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d398e-dadc-4d20-b41d-e0dcc1604618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_keys():\n",
    "    cursor.execute(\"SELECT key FROM documents WHERE text_nougat IS NOT NULL\")\n",
    "    return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "def query_document(doi):\n",
    "    cursor.execute(\"SELECT key, text, text_nougat, abstract, title, filepath, vhb_journal_title, vhb_issn FROM documents WHERE key=?\", (doi,))\n",
    "    doc = cursor.fetchone()\n",
    "    return {'doi': doc[0], 'text': doc[1], 'text_nougat': doc[2], 'abstract': doc[3], 'title': doc[4], 'filepath': doc[5], 'vhb_journal_title': doc[6], 'vhb_issn': doc[7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0bd9e-8387-4c00-bbd1-f7cc21f5efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dois_in_db = fetch_all_keys() # with this function you can find out all of the doi's that are included in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4fc92-ce03-48ef-9c4c-582366c12c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this function you can then get the document and for example look at the text and title specifically. Note that there are more datafields in the documents than text_nougat and title. Feel free to look at these.\n",
    "doi = '10.1287/isre.1110.0411'\n",
    "doc = query_document(doi)\n",
    "text_raw = doc['text_nougat']\n",
    "title = doc['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae0081-231c-40a0-bb29-5993a4d10a78",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Filtering for relevant sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b7a60-5f89-4e45-b3c9-aeedd6bba829",
   "metadata": {},
   "source": [
    "### Idea A. Using Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d7c4b-4ce7-4da2-95c5-dac9c7b93633",
   "metadata": {},
   "source": [
    "Here we use these bge-base-en-v1.5 embedding model. If you want to, you can also take any other embedding model, e.g. from this benchmark: https://huggingface.co/spaces/mteb/leaderboard. But the bge-base model is already very powerful.\n",
    "\n",
    "Embeddings in general are very powerful for finding data in text as you have already learned in the seminar. Feel free to try to find a better process with the embeddings by using a different model, pre-processing of the text or another query instead of the basic one we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23840b-dba8-4470-9330-16fa72ec2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain -qq\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ea3d5-386e-4d6b-b192-b3a1137d02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name='BAAI/bge-base-en-v1.5',\n",
    "                                                      model_kwargs={\"device\": \"cuda\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ba80c-c63a-4f35-89d2-61c32916707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "      separator = \".\",\n",
    "      chunk_size = 500,\n",
    "      chunk_overlap  = 100,\n",
    "      length_function = len,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1168f337-8e3e-4d43-8786-4a435771a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = text_splitter.split_text(text_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba69f47-0f95-4694-a7b4-4c9c818e83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_texts(text_split, instructor_embeddings)\n",
    "QUERY = 'we use machine learning model'\n",
    "text_relevant = [page.page_content for page in docsearch.similarity_search(QUERY, k=5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a1a0d-5f89-4ee0-9069-a5fdcb8d7cc2",
   "metadata": {},
   "source": [
    "### Idea B. Using Keyword Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef14c4-a6fc-4be5-a722-2a32d1f558b5",
   "metadata": {},
   "source": [
    "Here we use a self-built function to look through a sentence if it has a specific combination of keywords in it. Feel free to improve it, if you would like to try this direction. This basic version is likely pretty bad in finding important sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b5f85-2b8d-4a60-a7c6-1587009b897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(sentences):\n",
    "    sentences = sentences.lower()\n",
    "    relevant_keywords = [\n",
    "        'i', 'our', 'we'\n",
    "    ]\n",
    "    \n",
    "    search_keywords = ['machine learning']\n",
    "    \n",
    "    has_relevant_keywords = any(word in sentences for word in relevant_keywords)\n",
    "    has_search_keywords = any(word in sentences for word in search_keywords)\n",
    "    \n",
    "    return has_relevant_keywords and has_search_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17fdab-9ce5-446e-ae11-ded675fe662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = text_raw.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b55364-0693-403b-97db-de6f1f212716",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_relevant = [sentence.strip() for sentence in text_split if classify_sentence(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f0602c-5707-48e1-a07c-9a8e64f0213f",
   "metadata": {},
   "source": [
    "### Idea C. Other Possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125b651-d4f8-4791-8a89-139c445ae42a",
   "metadata": {},
   "source": [
    "Instead of using embeddings or using keywords to filter for specific sentences, you could for example use specific parts of the text (for example the abstract of the article).\n",
    "\n",
    "Other ideas could be to use a classification model to look for which sentences might be relevant. Google around or try to think about how you could find the few very helpful sentences in an article about the thing you want to know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce689bfa-d4b2-4c9d-b72c-2984a0c5b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using the abstract\n",
    "text_relevant = doc['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b0b84c-7bfa-4586-870a-db5d00a5b74c",
   "metadata": {},
   "source": [
    "The abstract of the article is a special thing, that we luckily have already in the database. If you want to use other sections e.g. like the method section of the article, you need to find a more creative solution to get to the section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2906a26c-17e8-4ade-90fc-d35db8425bfb",
   "metadata": {},
   "source": [
    "## 3. Extract an entity from the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a7b7c-1a3e-4cef-86e4-6c2b25c9b821",
   "metadata": {},
   "source": [
    "Here we use a large language model to extract the names of the machine learning models. Some interesting ideas to improve this step are for example to write a better prompt instead of the current one, our to fine-tune the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43da89a-220e-45b0-8eb5-86824beec619",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers>=4.32.0 optimum>=1.12.0 accelerate -qq\n",
    "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f1b01-6ccf-466f-bfef-aa5ed5ed2627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/OpenOrca-Platypus2-13B-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f353c-df60-4f37-a0c6-3df0294b87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=f'''### Instruction:\n",
    "Text: \"{\"\".join(text_relevant)}\"\n",
    "\n",
    "What machine learning models are used in the text? Return the data in JSON format.\n",
    "### Response:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ad918-f35b-4e62-938b-f2601aeca8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe(prompt_template)[0]['generated_text'].split(\"### Response:\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b5fb7-9e96-4846-9225-bcaa4faa8ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
